{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from keras.datasets import fashion_mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, layer_sizes, learning_rate=0.01):\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weights = {}\n",
    "        self.biases = {}\n",
    "        self.initialize_parameters()\n",
    "    \n",
    "    def initialize_parameters(self):\n",
    "        for i in range(1, len(self.layer_sizes)):\n",
    "            self.weights[i] = np.random.randn(self.layer_sizes[i], self.layer_sizes[i-1]) * 0.01\n",
    "            self.biases[i] = np.zeros((self.layer_sizes[i], 1))\n",
    "    \n",
    "    def softmax(self, z):\n",
    "        exp_z = np.exp(z - np.max(z, axis=0, keepdims=True))\n",
    "        return exp_z / np.sum(exp_z, axis=0, keepdims=True)\n",
    "    \n",
    "    def relu(self, z):\n",
    "        return np.maximum(0, z)\n",
    "    \n",
    "    def relu_derivative(self, z):\n",
    "        return (z > 0).astype(float)\n",
    "    \n",
    "    def forward_propagation(self, X):\n",
    "        activations = {0: X}\n",
    "        pre_activations = {}\n",
    "        for i in range(1, len(self.layer_sizes)):\n",
    "            z = self.weights[i] @ activations[i-1] + self.biases[i]\n",
    "            pre_activations[i] = z\n",
    "            activations[i] = self.softmax(z) if i == len(self.layer_sizes) - 1 else self.relu(z)\n",
    "        return activations, pre_activations\n",
    "    \n",
    "    def compute_loss(self, Y_hat, Y):\n",
    "        m = Y.shape[1]\n",
    "        return -np.sum(Y * np.log(Y_hat + 1e-8)) / m\n",
    "    \n",
    "    def backward_propagation(self, activations, pre_activations, Y):\n",
    "        gradients = {}\n",
    "        m = Y.shape[1]\n",
    "        dz = activations[len(self.layer_sizes) - 1] - Y\n",
    "        for i in reversed(range(1, len(self.layer_sizes))):\n",
    "            gradients[f'dW{i}'] = dz @ activations[i-1].T / m\n",
    "            gradients[f'db{i}'] = np.sum(dz, axis=1, keepdims=True) / m\n",
    "            if i > 1:\n",
    "                dz = (self.weights[i].T @ dz) * self.relu_derivative(pre_activations[i-1])\n",
    "        return gradients\n",
    "    \n",
    "    def update_parameters(self, gradients):\n",
    "        for i in range(1, len(self.layer_sizes)):\n",
    "            self.weights[i] -= self.learning_rate * gradients[f'dW{i}']\n",
    "            self.biases[i] -= self.learning_rate * gradients[f'db{i}']\n",
    "    \n",
    "    def train(self, X, Y, epochs=100, batch_size=64):\n",
    "        losses = []\n",
    "        for epoch in range(epochs):\n",
    "            activations, pre_activations = self.forward_propagation(X)\n",
    "            loss = self.compute_loss(activations[len(self.layer_sizes)-1], Y)\n",
    "            losses.append(loss)\n",
    "            gradients = self.backward_propagation(activations, pre_activations, Y)\n",
    "            self.update_parameters(gradients)\n",
    "            if epoch % 10 == 0:\n",
    "                print(f'Epoch {epoch}, Loss: {loss:.4f}')\n",
    "        return losses\n",
    "    \n",
    "    def predict(self, X):\n",
    "        activations, _ = self.forward_propagation(X)\n",
    "        return np.argmax(activations[len(self.layer_sizes)-1], axis=0)\n",
    "\n",
    "# Load Data\n",
    "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n",
    "X_train, X_test = X_train.reshape(X_train.shape[0], -1) / 255.0, X_test.reshape(X_test.shape[0], -1) / 255.0\n",
    "y_train, y_test = pd.get_dummies(y_train).values.T, pd.get_dummies(y_test).values.T\n",
    "\n",
    "# Define model\n",
    "nn = NeuralNetwork([784, 128, 64, 10], learning_rate=0.01)\n",
    "losses = nn.train(X_train.T, y_train, epochs=100)\n",
    "\n",
    "# Plot loss\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Curve')\n",
    "plt.show()\n",
    "\n",
    "# Evaluate\n",
    "predictions = nn.predict(X_test.T)\n",
    "accuracy = np.mean(predictions == np.argmax(y_test, axis=0))\n",
    "print(f'Test Accuracy: {accuracy * 100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import wandb\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "\n",
    "# Initialize wandb\n",
    "wandb.init(project=\"fashion-mnist-nn\", name=\"simple-nn\")\n",
    "\n",
    "class SimpleNeuralNetwork:\n",
    "    def __init__(self, layers, learning_rate=0.01):\n",
    "        self.layers = layers\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weights = {}\n",
    "        self.biases = {}\n",
    "        self.init_params()\n",
    "    \n",
    "    def init_params(self):\n",
    "        for i in range(1, len(self.layers)):\n",
    "            self.weights[i] = np.random.randn(self.layers[i], self.layers[i-1]) * 0.01\n",
    "            self.biases[i] = np.zeros((self.layers[i], 1))\n",
    "    \n",
    "    def softmax(self, x):\n",
    "        exp_x = np.exp(x - np.max(x, axis=0, keepdims=True))\n",
    "        return exp_x / np.sum(exp_x, axis=0, keepdims=True)\n",
    "    \n",
    "    def relu(self, x):\n",
    "        return np.maximum(0, x)\n",
    "    \n",
    "    def relu_derivative(self, x):\n",
    "        return (x > 0).astype(float)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        activations = {0: X}\n",
    "        pre_activations = {}\n",
    "        for i in range(1, len(self.layers)):\n",
    "            z = self.weights[i] @ activations[i-1] + self.biases[i]\n",
    "            pre_activations[i] = z\n",
    "            activations[i] = self.softmax(z) if i == len(self.layers) - 1 else self.relu(z)\n",
    "        return activations, pre_activations\n",
    "    \n",
    "    def loss(self, Y_hat, Y):\n",
    "        m = Y.shape[1]\n",
    "        return -np.sum(Y * np.log(Y_hat + 1e-8)) / m\n",
    "    \n",
    "    def backward(self, activations, pre_activations, Y):\n",
    "        grads = {}\n",
    "        m = Y.shape[1]\n",
    "        dz = activations[len(self.layers) - 1] - Y\n",
    "        for i in reversed(range(1, len(self.layers))):\n",
    "            grads[f'dW{i}'] = dz @ activations[i-1].T / m\n",
    "            grads[f'db{i}'] = np.sum(dz, axis=1, keepdims=True) / m\n",
    "            if i > 1:\n",
    "                dz = (self.weights[i].T @ dz) * self.relu_derivative(pre_activations[i-1])\n",
    "        return grads\n",
    "    \n",
    "    def update(self, grads):\n",
    "        for i in range(1, len(self.layers)):\n",
    "            self.weights[i] -= self.learning_rate * grads[f'dW{i}']\n",
    "            self.biases[i] -= self.learning_rate * grads[f'db{i}']\n",
    "    \n",
    "    def train(self, X, Y, epochs=100):\n",
    "        for epoch in range(epochs):\n",
    "            activations, pre_activations = self.forward(X)\n",
    "            loss = self.loss(activations[len(self.layers)-1], Y)\n",
    "            grads = self.backward(activations, pre_activations, Y)\n",
    "            self.update(grads)\n",
    "            wandb.log({\"epoch\": epoch, \"loss\": loss})\n",
    "            if epoch % 10 == 0:\n",
    "                print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
    "    \n",
    "    def predict(self, X):\n",
    "        activations, _ = self.forward(X)\n",
    "        return np.argmax(activations[len(self.layers)-1], axis=0)\n",
    "\n",
    "# Load data\n",
    "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n",
    "X_train = X_train.reshape(X_train.shape[0], -1) / 255.0\n",
    "X_test = X_test.reshape(X_test.shape[0], -1) / 255.0\n",
    "y_train = pd.get_dummies(y_train.flatten()).values.T\n",
    "y_test = pd.get_dummies(y_test.flatten()).values.T\n",
    "\n",
    "# Define model\n",
    "nn = SimpleNeuralNetwork([784, 128, 64, 10], learning_rate=0.01)\n",
    "nn.train(X_train.T, y_train, epochs=100)\n",
    "\n",
    "# Evaluate\n",
    "predictions = nn.predict(X_test.T)\n",
    "accuracy = np.mean(predictions == np.argmax(y_test, axis=0))\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
    "wandb.log({\"Test Accuracy\": accuracy * 100})\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
